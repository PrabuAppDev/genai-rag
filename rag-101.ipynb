{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207ccaeb-f77f-4bdc-9ca6-caccf5f62e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd4f7e1-9904-4e70-836f-25887c32b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad9587b-06ff-4386-b363-0af8401bd6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Load a smaller GPT-Neo model\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the device to CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946ee7ab-d443-41f4-8933-cb27597f74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Sentence Transformer model to convert sentences to embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Dummy corpus of documents for retrieval\n",
    "documents = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"The Great Wall of China is visible from space.\"\n",
    "    # \"Artificial Intelligence is transforming the world.\",\n",
    "    # \"The Grand Canyon is one of the seven wonders of the world.\"\n",
    "]\n",
    "\n",
    "# Convert the documents into embeddings\n",
    "document_embeddings = embedder.encode(documents)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = document_embeddings.shape[1]  # Dimension of the embeddings\n",
    "faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "faiss_index.add(np.array(document_embeddings))  # Add document embeddings to index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2639d75a-bc8d-4ac3-8668-822abd918ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response WITHOUT RAG:\n",
      "Where is the Eiffel Tower located?\n",
      "\n",
      "The Eiffel Tower is located in the city of Berlin, Germany. The tower is located in the city of Berlin, Germany. The tower is a part of the city of Berlin, Germany\n",
      "\n",
      "Response WITH RAG:\n",
      "Using the information from the document, answer the question.\n",
      "\n",
      "Document: The Eiffel Tower is located in Paris.\n",
      "\n",
      "Question: Where is the Eiffel Tower located?\n",
      "\n",
      "Answer: The Tower is in Paris, France.\n"
     ]
    }
   ],
   "source": [
    "def generate_without_rag(query):\n",
    "    \"\"\"Generate a response from the LLM without any external document retrieval.\"\"\"\n",
    "    # Tokenize the query and generate response\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output without any retrieved document\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def generate_with_rag(query):\n",
    "    \"\"\"Generate a response from the LLM with document retrieval using FAISS.\"\"\"\n",
    "    # Retrieve the relevant document using FAISS\n",
    "    query_embedding = embedder.encode([query])\n",
    "    _, indices = faiss_index.search(np.array(query_embedding), k=1)\n",
    "    relevant_document = documents[indices[0][0]]\n",
    "    \n",
    "    # Augment the query with the retrieved document\n",
    "    augmented_prompt = f\"Using the information from the document, answer the question.\\n\\nDocument: {relevant_document}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the augmented prompt and generate response\n",
    "    inputs = tokenizer(augmented_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3  # This prevents repeating the same n-grams\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "query = \"Where is the Eiffel Tower located?\"\n",
    "\n",
    "# Generate response without RAG\n",
    "response_without_rag = generate_without_rag(query)\n",
    "print(\"Response WITHOUT RAG:\")\n",
    "print(response_without_rag)\n",
    "\n",
    "# Generate response with RAG\n",
    "response_with_rag = generate_with_rag(query)\n",
    "print(\"\\nResponse WITH RAG:\")\n",
    "print(response_with_rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e60050d-04b5-45b8-8783-0b1859aa3946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved as acura_mdx_manual.pdf\n",
      "PDF text extracted.\n"
     ]
    }
   ],
   "source": [
    "def download_pdf(url, output_path='manual.pdf'):\n",
    "    response = requests.get(url)\n",
    "    with open(output_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"PDF downloaded and saved as {output_path}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PyPDF2.PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in range(len(reader.pages)):\n",
    "        text += reader.pages[page].extract_text()\n",
    "    return text\n",
    "\n",
    "# Download the PDF and extract text\n",
    "url = \"https://techinfo.honda.com/rjanisis/pubs/OM/AH/BTYA2222OM/enu/BTYA2222OM.pdf\"\n",
    "download_pdf(url, 'acura_mdx_manual.pdf')\n",
    "manual_text = extract_text_from_pdf('acura_mdx_manual.pdf')\n",
    "print(\"PDF text extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff185840-0eed-40fc-8e58-6acc4497c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual text chunked into 273 chunks.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"Chunk the text into smaller pieces for embedding.\"\"\"\n",
    "    text_chunks = []\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        text_chunks.append(chunk)\n",
    "    return text_chunks\n",
    "\n",
    "# Chunk the extracted text\n",
    "text_chunks = chunk_text(manual_text)\n",
    "print(f\"Manual text chunked into {len(text_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e9a0c9-3c87-4084-ae14-32a5a6c73990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and indexed.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each text chunk\n",
    "chunk_embeddings = embedder.encode(text_chunks)\n",
    "\n",
    "# Store embeddings using FAISS for efficient similarity search\n",
    "dimension = chunk_embeddings.shape[1]  # Dimension of the embeddings\n",
    "faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "faiss_index.add(np.array(chunk_embeddings))\n",
    "\n",
    "print(\"Embeddings created and indexed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "044a9362-6d4b-4439-9f82-432fbf4abb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_llm(query, context):\n",
    "    \"\"\"Generate a response based on the relevant manual chunk using the LLM.\"\"\"\n",
    "    prompt = f\"Using the information from the owner's manual section below, answer the question concisely:\\n\\nManual Section: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Set pad_token to eos_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the input and set attention_mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Set the `pad_token_id` to `eos_token_id`\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Generate the response with limited new tokens, attention mask, and no_repeat_ngram_size to prevent repetitions\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        attention_mask=attention_mask, \n",
    "        max_new_tokens=100,  # Control the number of new tokens generated\n",
    "        pad_token_id=pad_token_id,\n",
    "        no_repeat_ngram_size=3  # Prevent repetition of 3-grams\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d5bc9d7-5769-484f-afed-630b86753f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Using the information from the owner's manual section below, answer the question concisely:\n",
      "\n",
      "Manual Section: beam: All of the following conditions must be met before the high beams turn on. ●Your vehicle speed is 25mph (40 km/h) or more. ●There are no preceding or oncoming vehicle with headlights or taillights turned on. ●There are few street lights on the road ahead.One of the following conditions must be met before the low beams turn on. ●Your vehicle speed is 15 mph (24 km/h) or less. ●There is a preceding or oncoming vehicle with headlights or taillights turned on. ●There are many street lights on the road ahead.1How to Use the Auto High-Beam In the following cases, th e auto high-beam system may not switch the head lights properly or the switching timing may be ch anged. In case of the automatic switching operati on does not fit for your driving habits, please swit ch the headlights manually. •The brightness of the lights from the preceding or oncoming vehicle is intense or poor. •Visibility is poor due to the weather (rain, snow, fog, windshield frost, etc.). •Surrounding light sources, such as street lights, electric billboards and traf fic lights are illuminating the road ahead. •The brightness level of th e road ahead constantly changes. •The road is bumpy or has many curves. •A vehicle suddenly appears in front of you, or a vehicle in front of you is not in the preceding or oncoming direction. •Your vehicle is tilted with a heavy load in the rear. •A traffic sign, mirror, or other reflective object ahead is reflecting strong light toward the vehicle. •The oncoming vehicle freq uently disappears under roadside trees or be hind median barriers. •The preceding or oncoming vehicle is a motorcycle, bicycle, mobility scooter, or other small vehicle. The auto high-beam system keeps the headlight low beam when: •Windshield wipers are op erating at a high speed. •The camera has detected dense fog.22 ACURA MDX-31TYA6011.book 179 ページ ２０２１年１２月１４日 火曜日 午前１０時４２分uuOperating the Switches Around the Steering Wheel uAuto High-Beam 180Controls You can turn the auto high-beam system off. If you want to turn the system off or on, set the power mode to ON, then carry out the following procedures while the vehicle is stationary. To turn the system off:With the light switch is in AUTO, pull the lever toward you and hold it for at least 40 seconds. After the auto high-beam indicator light blinks twice, release the lever. To turn the system on: With the light switch is in AUTO, pull the lever toward you and hold it for at least 30 seconds. After the auto high-beam indicator light blinks once, release the lever.■How to Turn Off the Auto High-Beam1How to Use the Auto High-Beam If the Some driver assist systems cannot operate: Camera temperature too high message appears: •Use the climate control system to cool down the interior and, if necessary, also use de froster mode with the airflow directed toward the camera. •Start driving the vehicle to lower the windshield temperature, which cool s down the area around the\n",
      "\n",
      "Question: When does the Auto high beam mode turn ON?\n",
      "\n",
      "Answer: When the auto low-beam mode turns ON, the auto High-beam switch is turned OFF.\n",
      "\n",
      "How to turn off the Auto Low-Beams When the Auto low-beams are turned ON, you can turn off Auto High Beam.\n",
      "1How To Turn Off Auto High Beams When The Auto High beam is turned ON.\n",
      "2How To turn off auto High Beam When the High beam turns ON. The Auto high-beam switch is not turned OFF, but\n"
     ]
    }
   ],
   "source": [
    "def search_owners_manual(query, top_k=1):\n",
    "    \"\"\"Search for the most relevant chunk based on the query in the owner's manual.\"\"\"\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = faiss_index.search(np.array(query_embedding), top_k)\n",
    "    results = [text_chunks[idx] for idx in indices[0]]\n",
    "    return results\n",
    "\n",
    "# Use the existing function `generate_response_with_llm` to generate responses\n",
    "\n",
    "def query_acura_manual(query):\n",
    "    \"\"\"Query the Acura MDX owner's manual using embeddings and generate a response with the LLM.\"\"\"\n",
    "    # Search the owner's manual for relevant chunks\n",
    "    result_chunks = search_owners_manual(query)\n",
    "    context = result_chunks[0] if result_chunks else \"No relevant information found.\"\n",
    "    \n",
    "    # Generate a response using the existing LLM function\n",
    "    response = generate_response_with_llm(query, context)\n",
    "    return response\n",
    "\n",
    "# Example query\n",
    "user_query = \"When does the Auto high beam mode turn ON?\"\n",
    "response = query_acura_manual(user_query)\n",
    "print(f\"Final Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
