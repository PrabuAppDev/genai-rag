{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adb81c0-3e4f-4279-b7b6-acc6cc8b3635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (1.51.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2d8a81-97fc-44bd-ba26-66ed1a7fe407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from openai import OpenAI\n",
    "# https://platform.openai.com/docs/api-reference/debugging-requests?lang=python\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # This is the default and can be omitted\n",
    "#     api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Say this is a test\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"gpt-4o-mini\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8d21db-9353-4f7f-9352-3b9952a73240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def generate_response(messages, model=\"gpt-4o-mini\", max_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    General function to generate a response from OpenAI using the messages format.\n",
    "    Uses gpt-4o-mini as the default model.\n",
    "    \n",
    "    Parameters:\n",
    "    - messages: List of messages (chat format) where each message is a dict with \"role\" and \"content\"\n",
    "    - model: The model to be used (default is gpt-4o-mini)\n",
    "    - max_tokens: Maximum number of tokens to generate\n",
    "    - temperature: Sampling temperature (creativity level)\n",
    "    \n",
    "    Returns:\n",
    "    - Generated response as a string\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # Default to gpt-4o-mini\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Access the message content correctly using dot notation\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage of the generate function with different prompt patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83bb6f21-a683-4fbb-a7d3-4d76c5e4cd7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Response: The word 'cat' in French is 'chat'.\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot Learning\n",
    "# This method provides no examples and asks the model to generate a response based on its prior knowledge.\n",
    "messages = [{\"role\": \"user\", \"content\": \"Translate the word 'cat' into French.\"}]\n",
    "response = generate_response(messages)\n",
    "print(f\"Zero-Shot Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86599a95-190e-4225-a526-9bc082b440d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Response: The word 'dog' in French is 'chien'.\n"
     ]
    }
   ],
   "source": [
    "# One-Shot Learning\n",
    "# This method provides one example and asks the model to use that example to guide its answer.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Example: cat -> chat.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate the word 'dog' into French.\"}\n",
    "]\n",
    "response = generate_response(messages)\n",
    "print(f\"One-Shot Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbab6e63-c9e7-4028-a7c0-2ed3e9ee96d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Response: The word 'bird' in French is 'oiseau'.\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Learning\n",
    "# This method provides multiple examples (usually 2-5) and asks the model to apply the pattern seen in the examples.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Examples: cat -> chat, dog -> chien.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate the word 'bird' into French.\"}\n",
    "]\n",
    "response = generate_response(messages)\n",
    "print(f\"Few-Shot Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a999e15-3f32-4112-a577-fdea0e6fe6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot with Instruction Response: House in French is \"maison.\"\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot with Instructional Prompt\n",
    "# This method provides examples along with explicit instructions on how the model should behave.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Please translate the following words from English to French.\\nExamples: cat -> chat, dog -> chien.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate the word 'house' into French.\"}\n",
    "]\n",
    "response = generate_response(messages)\n",
    "print(f\"Few-Shot with Instruction Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265e457f-d783-4eda-befd-6df058cca048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought Response: 12 times 14 is 168.\n"
     ]
    }
   ],
   "source": [
    "# Chain-of-Thought Prompting\n",
    "# This method encourages the model to work through a problem step-by-step, mimicking how humans reason through tasks.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"First, multiply 10 by 14 to get 140. Then, multiply 2 by 14 to get 28. Now, add 140 and 28.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 12 times 14?\"}\n",
    "]\n",
    "response = generate_response(messages)\n",
    "print(f\"Chain-of-Thought Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0a3ced0-bbef-4d3c-a210-3d8ab1154285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought with Few-Shot Response: To solve 13 times 15, we can break it down as follows:\n",
      "\n",
      "First, multiply 10 by 15 to get 150.  \n",
      "Then, multiply 3 by 15 to get 45.  \n",
      "\n",
      "Now, add 150 and 45 to get:\n",
      "\n",
      "150 + 45 = 195.\n",
      "\n",
      "So, 13 times 15 equals 195.\n"
     ]
    }
   ],
   "source": [
    "# Chain-of-Thought with Few-Shot Learning\n",
    "# This method combines examples with step-by-step reasoning to help the model understand and process more complex tasks.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Example: 12 times 14: First, multiply 10 by 14 to get 140. Then, multiply 2 by 14 to get 28. Now, add 140 and 28 to get 168.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Now, solve 13 times 15.\"}\n",
    "]\n",
    "response = generate_response(messages)\n",
    "print(f\"Chain-of-Thought with Few-Shot Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ad3468-325c-4088-b187-e80bb438f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-Tuned Prompt Response: The Eiffel Tower, constructed in 1889, is situated in Paris.\n"
     ]
    }
   ],
   "source": [
    "# Instruction-Tuning and Custom Prompting\n",
    "# This method gives explicit instructions without examples, to guide the model's response based solely on those instructions.\n",
    "messages = [{\"role\": \"user\", \"content\": \"Summarize the following text in one sentence: 'The Eiffel Tower is located in Paris and was built in 1889.'\"}]\n",
    "response = generate_response(messages)\n",
    "print(f\"Instruction-Tuned Prompt Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
